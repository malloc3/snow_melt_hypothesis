{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cad662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is to take a CSV of specific format (illustrated below) and create figures from those data.\n",
    "#  The data in the CSV should be either sweHybrid, swe, or melt.\n",
    "#  Although the display/management of the data for all three types is pretty similar, MELT should be displayed\n",
    "#  slightly differently than sweHybrid or swe.    Special MELT scripts will be written for this management\n",
    "#\n",
    "#\n",
    "#    Each row should be a different pond.    There is no label for the ponds in the CSV the meta data should be\n",
    "#        handled carefully there.\n",
    "#    The first row is the date values\n",
    "#    Each column is a new day   with the first row of each column being the date (in matlab dates cause hecc)\n",
    "#\n",
    "#    date, date, date, date\n",
    "#    value_p1, value_p1, value_p1, value_p1\n",
    "#    value_p2, value_p2, value_p2, value_p2\n",
    "#    value_p3, value_p3, value_p3, value_p3\n",
    "#\n",
    "# Each value should be some combination of total melt across the area of the pond.  Either Sum, average, or \n",
    "#    Otherwise.   This script will have absolutly no vision into how those data are calculated so be smart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beda8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcf9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was copied from https://gist.github.com/victorkristof/b9d794fe1ed12e708b9d\n",
    "# seemed legit but I havent actually checked their work.   I think it should do the trick tho\n",
    "def datenum_to_datetime(datenum):\n",
    "    \"\"\"\n",
    "    Convert Matlab datenum into Python datetime.\n",
    "    :param datenum: Date in datenum format\n",
    "    :return:        Datetime object corresponding to datenum.\n",
    "    \"\"\"\n",
    "    days = datenum % 1\n",
    "    return datetime.date.fromordinal(int(datenum)) \\\n",
    "           + datetime.timedelta(days=days) \\\n",
    "           - datetime.timedelta(days=366)\n",
    "\n",
    "#The name of this function is intentionally obscure to limit it's use.   It is not very universal\n",
    "# But it works for my initial 26 ponds assuming they are in a consistant order.   This is not garunteed for\n",
    "# all ponds though so this method should be used VERY sparingly.   Mostly just creates my initial dataframes\n",
    "# Gets the raw data from the CSV file and organizes it into a pandas df for later\n",
    "#\n",
    "def custom_function_that_is_specific_to_data_type(pond_df, raw_data_csv_file, raw_data_df, data_key):\n",
    "    print('this function takes a while.... Sorry it has for loops')\n",
    "    raw_pond_file = open(raw_data_csv_file)\n",
    "    raw_pond_data_reader = csv.reader(raw_pond_file)\n",
    "    for idx, row in enumerate(raw_pond_data_reader):\n",
    "        if idx == 0:\n",
    "            dates = list(map(datenum_to_datetime, \n",
    "                             list(map(float, row)))) #creates datetime list in python format\n",
    "            continue\n",
    "        site_data = list(map(float, row))\n",
    "        site_id = pond_df['site_id'].iloc[idx-1]\n",
    "        for date, site_value in zip(dates, site_data):\n",
    "            temporary_dict = {\"site_id\": site_id, \"date\": date, data_key: site_value, \n",
    "                             \"units\": 'mm', \"area_size_m\": 500, \n",
    "                              \"averaging_method\": 'sum'}\n",
    "            raw_data_df = raw_data_df.append(temporary_dict, ignore_index=True)\n",
    "    return(raw_data_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c26a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta data should ultimatly be included in the raw data file instead of two separate files...\n",
    "# These should be changed each time too.\n",
    "swe_hybrid_raw_data_csv_file = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/Pond_csv_data/01-Dec-2023sweHybrid.csv\"\n",
    "melt_raw_data_csv = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/Pond_csv_data/04-Dec-2023melt.csv\"\n",
    "swe_raw_data_csv = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/Pond_csv_data/04-Dec-2023swe.csv\"\n",
    "pond_meta_data_csv = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/Initial_Ponds.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b9e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eventually these will be the same cause we just want to update the old one.\n",
    "# But we are being very careful with this because we don't wanna overwrite out data\n",
    "#\n",
    "# Will need to change these each time to the most recent exsiting DF and the new name for the next df\n",
    "save_all_data_csv = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/DataFrame_CSV/dec_5_all_melt.csv\" # Where we will save the old df\n",
    "existing_df = \"/Users/Cannon/Documents/School/UCSB/Briggs Lab/Thaw_Rate_Hypothesis/Raw Snow Melt Data (Bair et. Al) /Points_OF_Interest/DataFrame_CSV/dec_4_2023_all_melt.csv\"  #where the old exising DF is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e247490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gest meta data from meta data file and creates the master data frame\n",
    "meta_pond_file = open(pond_meta_data_csv)\n",
    "meta_pond_data_reader = csv.reader(meta_pond_file)\n",
    "meta_pond_data = []\n",
    "keys = []\n",
    "for idx, row in enumerate(meta_pond_data_reader):\n",
    "    if idx == 0:\n",
    "        keys = list(row)\n",
    "        continue\n",
    "    meta_pond_data.append(list(row))\n",
    "pond_df = pd.DataFrame(meta_pond_data, columns = keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d753de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f294310",
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_type_columns = [\"site_id\", \"date\", \"value\", \"units\", \"area_size_m\", \"averaging_method\"]\n",
    "sweHybrid_df = pd.DataFrame(columns = melt_type_columns)\n",
    "swe_df = pd.DataFrame(columns = melt_type_columns)\n",
    "melt_df = pd.DataFrame(columns = melt_type_columns)\n",
    "\n",
    "sweHybrid_df = custom_function_that_is_specific_to_data_type(pond_df, \n",
    "                                                             swe_hybrid_raw_data_csv_file, \n",
    "                                                             sweHybrid_df,\n",
    "                                                             'sweHybrid')\n",
    "melt_df = custom_function_that_is_specific_to_data_type(pond_df, \n",
    "                                                             melt_raw_data_csv, \n",
    "                                                             melt_df,\n",
    "                                                             'melt')\n",
    "swe_df = custom_function_that_is_specific_to_data_type(pond_df, \n",
    "                                                             swe_raw_data_csv, \n",
    "                                                             swe_df,\n",
    "                                                             'swe')\n",
    "\n",
    "all_data = pd.merge(sweHybrid_df, melt_df, on=[\"site_id\", \"date\"], suffixes=('', '_df2'))\n",
    "columns_to_delete = ['value_df2', 'units_df2', 'area_size_m_df2', 'averaging_method_df2']\n",
    "all_data = all_data.drop(columns=columns_to_delete)\n",
    "\n",
    "all_data = pd.merge(all_data, swe_df, on=[\"site_id\", \"date\"], suffixes=('', '_df2'))\n",
    "columns_to_delete = ['value_df2', 'units_df2', 'area_size_m_df2', 'averaging_method_df2']\n",
    "all_data = all_data.drop(columns=columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6efd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we should have all the data from the new DFs so all we gotta do now is merge that with \n",
    "# The existing dataframe.)\n",
    "site_data = pd.read_csv(existing_df)\n",
    "site_data = site_data.append(all_data, ignore_index = True) #Should append the two dataframes\n",
    "#This section will check the df for repeats\n",
    "duplicates = df[df.duplicated(subset=['ID', 'Value'], keep=False)]\n",
    "duplicates = duplicates['Value'].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not duplicates:\n",
    "    site_data.to_csv(save_all_data_csv, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
